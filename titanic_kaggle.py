# -*- coding: utf-8 -*-
"""titanic_kaggle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y0gMrt_eiDWSYLn5eCv1kaZrYjOvxJVc
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

# This function is for dataset preprocessing. Feature Engineering and manipulation or categorical to numerical changes are done in this function.
# Parameters: location - Location of the dataset if it is test or train data; isTest - If it is test dataset or train dataset
# Returns: If isTest is true, returns data with dropped columns; else returns data
def datasetPreprocessing(location, isTest):

	# Read csv file using pd.read_csv from location
	data = pd.read_csv(location)

	# Extract title from Name
	data['title'] = data['Name'].str.extract('([A-Za-z]+)\.', expand=False)
	# Assigning all the odd or rare titles to odd
	data['title'] = data['title'].replace(['Lady', 'Countess','Capt', 'Col',
 	    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Odd')
	# Assigning Mlle(Mademoiselle meaning unmarried women in French) and 'Ms' to 'Miss'
	data['title'] = data['title'].replace(['Mlle', 'Ms'], 'Miss')
	# Assigning Mme(maried women in French) to 'Mrs'
	data['title'] = data['title'].replace(['Mme'], 'Mrs')
	# Giving values to each title
	title_map = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Odd": 5}
	# Mapping the title with values
	data['title'] = data['title'].map(title_map)
	# Assigning the rest values with 0
	data['title'] = data['title'].fillna(0)

	# Assigning 1 to female and 0 to male
	data['Sex'] = data['Sex'].map( {'female': 1, 'male': 0} ).astype(int)
	
	# Calculating total_family present on the ship by adding the parameters SibSp(number of siblings / spouses aboard the Titanic)
	# and Parch (number of parents / children aboard the Titanic)
	data['total_family'] = data['SibSp'] + data['Parch']

	# Calculating Fare per person using total family and adding 1 to the denominator to count the passenger
	data['fare_per_person'] = data['Fare'] / (data['total_family'] + 1)
	# Adding a new column 'alone' by using total family
	data['alone'] = np.where((data['total_family']) > 0, 0, 1)

	# Mapping Embarked values C, Q, S to 1, 2, 3 to give it numerical values
	data['Embarked'] = data['Embarked'].map({"C": 1, "Q": 2, "S": 3})

	# Cleaning the column 'Age' by filling NaN values with 0
	data['Age'] = data['Age'].fillna(0)
	# Adding a new category 'age_group' and giving labels and used pd.cut to map/bin values into discrete intervals
	data['age_group'] = pd.cut(data['Age'], bins = [0, 12, 18, 65, np.inf], labels=['Children', 'Teenagers', 'Adults', 'Senior Citizens'])

	# Converting categorical variables into dummy variables. Here, for example, if the age_group has two values 'a' and 'b', pd.get_dummies creates
	# two columns 'age_group_a' and 'age_group_b' and assigns binary values to them.
	data = pd.get_dummies(data, columns=['age_group'])
	
	# Giving all the NaN values to 0
	data = data.replace(np.nan, 0)
	# Drop columns which are not necessary
	if isTest:
		data = data.drop(['Name', 'Ticket', 'Cabin', 'SibSp', 'Parch', 'Age'], axis = 1)		
	
	return data

def getTestPassengerId():
	test = pd.read_csv('./test.csv')
	return test['PassengerId']

# Giving parameter values n_estimators, mx_depth, min_samples_split, min_samples_leaf, max_features to GridSearchCV and RandomizedSearchCV
parameters = {'n_estimators': [50, 100, 200, 300],
              'max_depth': [2, 4, 6, 8],
              'min_samples_split': [2, 4, 6],
              'min_samples_leaf': [1, 2, 4],
              'max_features': ['auto', 'sqrt', 'log2']}

# Function for training the Random Forest Classifier
# Parameters: X_train, y_train
# Returns: model
def randomForest(X_train, y_train):
  model = RandomForestClassifier(max_depth = 4, max_features = 'auto', min_samples_leaf = 1, min_samples_split = 4, n_estimators = 300, random_state = 42)
  model.fit(X_train, y_train)
  return model

# Function for training the Logistic regression
# Parameters: X_train, y_train
# Returns: model
def LR(X_train, y_train):
  model = LogisticRegression()
  model.fit(X_train, y_train)
  return model

# Function for training the k-nearest neighbor classifier
# Parameters: X_train, y_train
# Returns: model
def KNN(X_train, y_train):
  model = KNeighborsClassifier(n_neighbors=3)
  # Train the model using the training sets
  model.fit(X_train, y_train)
  return model

# Function for training the Decision Tree classifier
# Parameters: X_train, y_train
# Returns: model
def DecisionTree(X_train, y_train):
  model = DecisionTreeClassifier(criterion="gini", random_state=42,max_depth=3, min_samples_leaf=5)   
  model.fit(X_train,y_train)
  return model

# Function for training the Support Vector Machine
# Parameters: X_train, y_train
# Returns: model
def svmModel(X_train, y_train):
  model = SVC()
  model.fit(X_train, y_train)
  return model

# Function for training the XGB Classifier
# Parameters: X_train, y_train
# Returns: model
def xgb(X_train, y_train):
  model = XGBClassifier(random_state=42)
  model.fit(X_train, y_train)
  return model

# Function for training the Random Forest Classifier with GridSearchCV
# Parameters: X_train, y_train
# Returns: model
def gridSearch(X_train, y_train):
  model = GridSearchCV(RandomForestClassifier(n_estimators=200, random_state=42), parameters, cv=5, scoring='accuracy')
  model.fit(X_train, y_train)
  return model

# Function for training the Random Forest Classifier with RandomizedSearchCV
# Parameters: X_train, y_train
# Returns: model
def randomSearch(X_train, y_train):
  model = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_distributions = parameters, n_iter=10, cv=5, scoring='accuracy', random_state=42)
  model.fit(X_train, y_train)
  return model

# Function for training the Random Forest Classifier with a Pipeline
# Parameters: X_train, y_train
# Returns: model
def pipelineModel(X_train, y_train):
  model = Pipeline([('scaler', StandardScaler()), ('classifier', RandomForestClassifier(n_estimators=200, random_state=42, max_depth=8, min_samples_split=6, min_samples_leaf=4))])
  model.fit(X_train, y_train)
  return model


test = datasetPreprocessing('./test.csv', True)
train = datasetPreprocessing('./train.csv', False)

y = train['Survived']
X = train.drop(['Survived', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch', 'Age'], axis=1)
print(X.describe)
print(test.describe)
# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Train the model
RFmodel = randomForest(X_train, y_train)
LRmodel = LR(X_train, y_train)
KNNmodel = KNN(X_train, y_train)
DTmodel = DecisionTree(X_train, y_train)
xgbModel = xgb(X_train, y_train)
randomSearchModel = randomSearch(X_train, y_train)
gridModel = gridSearch(X_train, y_train)
pipeline = pipelineModel(X_train, y_train)
svmModel = svmModel(X_train, y_train)

# Make predictions on test set
y_pred_RF = RFmodel.predict(X_test)
y_pred_LR = LRmodel.predict(X_test)
y_pred_KNN = KNNmodel.predict(X_test)
y_pred_DT = DTmodel.predict(X_test)
y_pred_xgb = xgbModel.predict(X_test)
y_pred_random = randomSearchModel.predict(X_test)
y_pred_new = gridModel.predict(X_test)
y_pred_svm = svmModel.predict(X_test)
y_pred_pipeline = pipeline.predict(X_test)

# For calculating RandomForest and XGB combined accuracy
final_pred = np.floor((y_pred_RF + y_pred_xgb) / 2)
final_pred.round().astype(np.int64)

print(f'Random Forest Classifier Accuracy: {accuracy_score(y_test, y_pred_RF)}')
print(f'Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_RF)}')
print(f'K-nearest Neighbor Accuracy: {accuracy_score(y_test, y_pred_RF)}')
print(f'Decision tree Accuracy: {accuracy_score(y_test, y_pred_RF)}')
print(f'XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb)}')
print(f'Random Forest and XGBoost combined Accuracy: {accuracy_score(y_test, final_pred)}')
print(f'Random Search Random Forest Accuracy: {accuracy_score(y_test, y_pred_random)}')
print(f'Grid Search Random Forest Accuracy: {accuracy_score(y_test, y_pred_new)}')
print(f'Support Vector Machine Accuracy: {accuracy_score(y_test, y_pred_svm)}')
print(f'Pipeline Accuracy: {accuracy_score(y_test, y_pred_pipeline)}')

test_pred_rf = RFmodel.predict(test)
test_pred_xgb = xgbModel.predict(test)
test_pred_lr = LRmodel.predict(test)
test_pred_KNN = KNNmodel.predict(test)
test_pred_DT = DTmodel.predict(test)
final_pred = np.floor((test_pred_rf + test_pred_xgb) / 2)
final_pred.round().astype(np.int64)
test_pred_grid = gridModel.predict(test)
test_pred_random = randomSearchModel.predict(test)
test_pred_svm = svmModel.predict(test)
test_pred_pipeline = pipeline.predict(test)

final = np.floor((test_pred_rf + test_pred_xgb) / 2)

test_predictions = pd.DataFrame({'Survived': test_pred_pipeline})
test_predictions["Survived"] = test_predictions["Survived"].apply(int)
test_predictions.index = getTestPassengerId()

test_predictions.to_csv('test_prediction.csv', index_label='PassengerId')